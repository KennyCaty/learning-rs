{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "import jieba\n",
    "\n",
    "# 中文测试\n",
    "doc = \"刚刚，理论计算机科学家、UT Austin 教授、量子计算先驱 Scott Aaronson 因其「对量子计算的开创性贡献」被授予 2020 年度 ACM 计算奖。在获奖公告中，ACM 表示：「量子计算的意义在于利用量子物理学定律解决传统计算机无法解决或无法在合理时间内解决的难题。Aaronson 的研究展示了计算复杂性理论为量子物理学带来的新视角，并清晰地界定了量子计算机能做什么以及不能做什么。他在推动量子优越性概念发展的过程起到了重要作用，奠定了许多量子优越性实验的理论基础。这些实验最终证明量子计算机可以提供指数级的加速，而无需事先构建完整的容错量子计算机。」 ACM 主席 Gabriele Kotsis 表示：「几乎没有什么技术拥有和量子计算一样的潜力。尽管处于职业生涯的早期，但 Scott Aaronson 因其贡献的广度和深度备受同事推崇。他的研究指导了这一新领域的发展，阐明了它作为领先教育者和卓越传播者的可能性。值得关注的是，他的贡献不仅限于量子计算，同时也在诸如计算复杂性理论和物理学等领域产生了重大影响。」\"\n",
    "doc = \" \".join(jieba.cut(doc))\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "print(\"naive ...\")\n",
    "keywords = kw_model.extract_keywords(doc)\n",
    "print(keywords)\n",
    "\n",
    "print(\"\\nkeyphrase_ngram_range ...\")\n",
    "keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 2), stop_words=None)\n",
    "print(keywords)\n",
    "\n",
    "print(\"\\nhighlight ...\")\n",
    "keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 2), highlight=True)\n",
    "print(keywords)\n",
    "\n",
    "# 为了使结果多样化，我们将 2 x top_n 与文档最相似的词/短语。\n",
    "# 然后，我们从 2 x top_n 单词中取出所有 top_n 组合，并通过余弦相似度提取彼此最不相似的组合。\n",
    "print(\"\\nuse_maxsum ...\")\n",
    "keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 3), stop_words='english',\n",
    "                              use_maxsum=True, nr_candidates=20, top_n=5)\n",
    "print(keywords)\n",
    "\n",
    "# 为了使结果多样化，我们可以使用最大边界相关算法(MMR)\n",
    "# 来创建同样基于余弦相似度的关键字/关键短语。 具有高度多样性的结果：\n",
    "print(\"\\nhight diversity ...\")\n",
    "keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english',\n",
    "                        use_mmr=True, diversity=0.7)\n",
    "print(keywords)\n",
    "\n",
    "# 低多样性的结果\n",
    "print(\"\\nlow diversity ...\")\n",
    "keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english',\n",
    "                        use_mmr=True, diversity=0.2)\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关键词 不使用use_mmr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'lex', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '为什', '什', '倒', '傥', '元', '先', '兼', '前', '吨', '唷', '啪', '啷', '喔', '外', '多年', '大面儿', '天', '始', '後', '抗拒', '敞开', '数', '新', '日', '昉', '末', '次', '毫无保留', '漫', '特', '特别', '理', '皆', '目前为止', '策略', '设', '话', '说', '赶早', '赶晚', '达', '限', '非', '面', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('线性方程组', 0.6535), ('线性代数', 0.6431), ('线性', 0.6417), ('矩阵', 0.569), ('线性变换', 0.5431), ('基础理论', 0.5186), ('数学', 0.5038), ('算法', 0.5034), ('方法论', 0.4906), ('几何', 0.4673), ('代数和', 0.4574), ('特征值', 0.4541), ('编程', 0.45), ('框架', 0.4492), ('特征向量', 0.4433), ('ometrymetry', 0.4431), ('高中数学', 0.4417), ('机器学习', 0.4282), ('抽象思维', 0.4263), ('嵌入式', 0.4002), ('行列式', 0.3926), ('化', 0.3897), ('性质', 0.3862), ('数据结构', 0.3826), ('领域', 0.381), ('逻辑思维', 0.3806), ('三角', 0.3799), ('开发', 0.3777), ('人工智能', 0.3774), ('编程语言', 0.3747)]\n",
      "关键词 使用use_mmr...\n",
      "[('线性方程组', 0.6535), ('矩阵', 0.569), ('基础理论', 0.5186), ('算法', 0.5034), ('框架', 0.4492), ('特征向量', 0.4433), ('ometrymetry', 0.4431), ('高中数学', 0.4417), ('机器学习', 0.4282), ('抽象思维', 0.4263), ('三角', 0.3799), ('开发', 0.3777), ('编程语言', 0.3747), ('a1', 0.3649), ('分布式系统', 0.3648), ('拓宽', 0.3451), ('空间', 0.33), ('软件架构', 0.3243), ('专业知识', 0.3154), ('推荐', 0.2972), ('软件测试', 0.2815), ('线', 0.27), ('协作', 0.2681), ('环境', 0.2467), ('云计算', 0.2406), ('变化', 0.2336), ('数据库', 0.2237), ('意义', 0.2163), ('快速', 0.1525), ('网络安全', 0.1064)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba\n",
    "\n",
    "# 中文测试\n",
    "# doc_path = './testClass/cpp.txt'\n",
    "# doc_path = './testClass/电路与模拟电子技术.txt'\n",
    "doc_path = './testClass/线性代数A1.txt'\n",
    "# doc_path = './testClass/计算机网络.txt'\n",
    "# doc_path = './testClass/计算机组成原理.txt'\n",
    "# doc_path = './testClass/离散数学.txt'\n",
    "# doc_path = './testClass/操作系统.txt'\n",
    "# doc_path = './testClass/数据结构.txt'\n",
    "# doc_path = './testClass/软件工程.txt'\n",
    "\n",
    "# doc_path = './testRoute/计算机视觉.txt'\n",
    "\n",
    "doc = ''\n",
    "with open (doc_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        doc+=line\n",
    "\n",
    " \n",
    "jieba.load_userdict(r'stop_words\\vocab.txt') # 载入自定义的词典，定义一些专业性名词\n",
    "\n",
    "def tokenize_zh(text):\n",
    "    words = jieba.lcut(text, HMM=True)\n",
    "    return words\n",
    " \n",
    "# ======================== 获取停用词 ======================\n",
    "def load_stopwords(stopwords_file):\n",
    "    stopwords = set()   # 为了去重\n",
    "    with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            stopwords.add(line.strip())\n",
    "    stopwords = list(stopwords)\n",
    "    return stopwords\n",
    "\n",
    "stop_words = load_stopwords(r'stop_words\\myStopWords.txt')\n",
    "# ======================= END 获取停用词 ======================\n",
    "\n",
    "\n",
    "\n",
    "# 包含自己的词表的列表\n",
    "# vectorizer1 = CountVectorizer(tokenizer=tokenize_zh)  \n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_zh, stop_words=stop_words)  # 自定义了vectorizer之后，停用词加载这！！！！ \n",
    "\n",
    "\n",
    "kw_model = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')  # 官方说英文以外的其他语言用这个模型\n",
    "\n",
    "\n",
    "print(\"关键词 不使用use_mmr...\")\n",
    "keywords = kw_model.extract_keywords(doc, vectorizer=vectorizer, top_n=30, diversity=0.8, use_mmr=False)\n",
    "print(keywords)\n",
    "\n",
    "print(\"关键词 使用use_mmr...\")\n",
    "keywords = kw_model.extract_keywords(doc, vectorizer=vectorizer, top_n=30, diversity=0.8,  use_mmr=True) # 传入了分词器，keyphrase_ngram_range=(1, 3),就没有用了， 可在CountVectorizer里面设置ngram_range\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 所有实体的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理： 云计算.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'lex', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '为什', '什', '倒', '傥', '元', '先', '兼', '前', '吨', '唷', '啪', '啷', '喔', '外', '多年', '大面儿', '天', '始', '後', '抗拒', '敞开', '数', '新', '日', '昉', '末', '次', '毫无保留', '漫', '特', '特别', '理', '皆', '目前为止', '策略', '设', '话', '说', '赶早', '赶晚', '达', '限', '非', '面', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理： 产品运维.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'lex', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '为什', '什', '倒', '傥', '元', '先', '兼', '前', '吨', '唷', '啪', '啷', '喔', '外', '多年', '大面儿', '天', '始', '後', '抗拒', '敞开', '数', '新', '日', '昉', '末', '次', '毫无保留', '漫', '特', '特别', '理', '皆', '目前为止', '策略', '设', '话', '说', '赶早', '赶晚', '达', '限', '非', '面', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理： 人工智能.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'lex', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '为什', '什', '倒', '傥', '元', '先', '兼', '前', '吨', '唷', '啪', '啷', '喔', '外', '多年', '大面儿', '天', '始', '後', '抗拒', '敞开', '数', '新', '日', '昉', '末', '次', '毫无保留', '漫', '特', '特别', '理', '皆', '目前为止', '策略', '设', '话', '说', '赶早', '赶晚', '达', '限', '非', '面', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理： 前端开发.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'lex', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '为什', '什', '倒', '傥', '元', '先', '兼', '前', '吨', '唷', '啪', '啷', '喔', '外', '多年', '大面儿', '天', '始', '後', '抗拒', '敞开', '数', '新', '日', '昉', '末', '次', '毫无保留', '漫', '特', '特别', '理', '皆', '目前为止', '策略', '设', '话', '说', '赶早', '赶晚', '达', '限', '非', '面', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理： 后端开发.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'lex', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '为什', '什', '倒', '傥', '元', '先', '兼', '前', '吨', '唷', '啪', '啷', '喔', '外', '多年', '大面儿', '天', '始', '後', '抗拒', '敞开', '数', '新', '日', '昉', '末', '次', '毫无保留', '漫', '特', '特别', '理', '皆', '目前为止', '策略', '设', '话', '说', '赶早', '赶晚', '达', '限', '非', '面', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理： 游戏开发.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'lex', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '为什', '什', '倒', '傥', '元', '先', '兼', '前', '吨', '唷', '啪', '啷', '喔', '外', '多年', '大面儿', '天', '始', '後', '抗拒', '敞开', '数', '新', '日', '昉', '末', '次', '毫无保留', '漫', '特', '特别', '理', '皆', '目前为止', '策略', '设', '话', '说', '赶早', '赶晚', '达', '限', '非', '面', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理： 硬件开发.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'lex', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '为什', '什', '倒', '傥', '元', '先', '兼', '前', '吨', '唷', '啪', '啷', '喔', '外', '多年', '大面儿', '天', '始', '後', '抗拒', '敞开', '数', '新', '日', '昉', '末', '次', '毫无保留', '漫', '特', '特别', '理', '皆', '目前为止', '策略', '设', '话', '说', '赶早', '赶晚', '达', '限', '非', '面', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理： 移动开发.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'lex', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '为什', '什', '倒', '傥', '元', '先', '兼', '前', '吨', '唷', '啪', '啷', '喔', '外', '多年', '大面儿', '天', '始', '後', '抗拒', '敞开', '数', '新', '日', '昉', '末', '次', '毫无保留', '漫', '特', '特别', '理', '皆', '目前为止', '策略', '设', '话', '说', '赶早', '赶晚', '达', '限', '非', '面', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理： 算法设计.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'lex', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '为什', '什', '倒', '傥', '元', '先', '兼', '前', '吨', '唷', '啪', '啷', '喔', '外', '多年', '大面儿', '天', '始', '後', '抗拒', '敞开', '数', '新', '日', '昉', '末', '次', '毫无保留', '漫', '特', '特别', '理', '皆', '目前为止', '策略', '设', '话', '说', '赶早', '赶晚', '达', '限', '非', '面', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理： 网络开发.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'lex', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '为什', '什', '倒', '傥', '元', '先', '兼', '前', '吨', '唷', '啪', '啷', '喔', '外', '多年', '大面儿', '天', '始', '後', '抗拒', '敞开', '数', '新', '日', '昉', '末', '次', '毫无保留', '漫', '特', '特别', '理', '皆', '目前为止', '策略', '设', '话', '说', '赶早', '赶晚', '达', '限', '非', '面', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理： 计算机视觉.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'lex', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '为什', '什', '倒', '傥', '元', '先', '兼', '前', '吨', '唷', '啪', '啷', '喔', '外', '多年', '大面儿', '天', '始', '後', '抗拒', '敞开', '数', '新', '日', '昉', '末', '次', '毫无保留', '漫', '特', '特别', '理', '皆', '目前为止', '策略', '设', '话', '说', '赶早', '赶晚', '达', '限', '非', '面', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "d:\\CodeSoftware\\Anaconda\\envs\\d2l\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keybert import KeyBERT\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "VOCAB_PATH = 'stop_words/vocab.txt'\n",
    "STOP_WORDS_PATH = 'stop_words/myStopWords.txt'\n",
    "TOP_N = 30\n",
    "\n",
    "\n",
    "# 指定保存的课程信息路径\n",
    "# folder_path = r'testClass'\n",
    "folder_path = r'testRoute'\n",
    "\n",
    "# 遍历文件夹下的所有文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    # 检查文件是否是txt文件\n",
    "    if filename.endswith('.txt'):\n",
    "        # 构建完整的文件路径\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # 打开并读取txt文件的内容\n",
    "        with open(file_path, 'r') as file:\n",
    "            doc = file.read()\n",
    "            \n",
    "            # 处理文件\n",
    "            jieba.load_userdict(VOCAB_PATH) # 载入自定义的词典，定义一些专业性名词\n",
    "\n",
    "            def tokenize_zh(text):\n",
    "                words = jieba.lcut(text, HMM=True)\n",
    "                return words\n",
    "            \n",
    "            # ======================== 获取停用词 ======================\n",
    "            def load_stopwords(stopwords_file):\n",
    "                stopwords = set()   # 为了去重\n",
    "                with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        stopwords.add(line.strip())\n",
    "                stopwords = list(stopwords)\n",
    "                return stopwords\n",
    "\n",
    "            stop_words = load_stopwords(STOP_WORDS_PATH)\n",
    "            # ======================= END 获取停用词 ======================\n",
    "\n",
    "\n",
    "            # 包含自己的词表的列表\n",
    "            # vectorizer1 = CountVectorizer(tokenizer=tokenize_zh)  \n",
    "            vectorizer = CountVectorizer(tokenizer=tokenize_zh, stop_words=stop_words)  # 自定义了vectorizer之后，停用词加载这！！！！ \n",
    "\n",
    "\n",
    "            kw_model = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')  # 官方说英文以外的其他语言用这个模型\n",
    "\n",
    "\n",
    "\n",
    "            print(\"正在处理：\",filename)\n",
    "            # print(\"关键词 不使用use_mmr...\")\n",
    "            keywords1 = kw_model.extract_keywords(doc, vectorizer=vectorizer, top_n=TOP_N, diversity=0.8, use_mmr=False)\n",
    "            # print(keywords1)\n",
    "\n",
    "            # print(\"关键词 使用use_mmr...\")\n",
    "            keywords2 = kw_model.extract_keywords(doc, vectorizer=vectorizer, top_n=TOP_N, diversity=0.8,  use_mmr=True) # 传入了分词器，keyphrase_ngram_range=(1, 3),就没有用了， 可在CountVectorizer里面设置ngram_range\n",
    "            # print(keywords2)\n",
    "\n",
    "            # with open(f'./class_key/{filename[:-4]}.txt', 'w') as f:\n",
    "            with open(f'./route_key/{filename[:-4]}.txt', 'w') as f:\n",
    "                key_set = set()\n",
    "                for kw in keywords1:\n",
    "                    key_set.add(kw[0])\n",
    "                for kw in keywords2:\n",
    "                    key_set.add(kw[0])\n",
    "                key_list = list(key_set)\n",
    "                \n",
    "                for k in key_list:\n",
    "                    f.write(k + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
